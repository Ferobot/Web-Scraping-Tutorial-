{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de API de manera directa\n",
    "#### Sunset and sunrise times API\n",
    "Sirve para obtener la hora del amanecer y el ocaso de un determinado dia\n",
    "Parametro:\n",
    "* lat (float):Latitud en grados decimales (Obligatorio)\n",
    "* lng (float):Longitud en grados decimales (Obligatorio)\n",
    "* date(string): Fecha en formato AAAA-MM-DD (opcional, por defecto usa actual)\n",
    "\n",
    "Estructura de la query:\n",
    "\n",
    "https://api.sunrise-sunset.org/json\n",
    "\n",
    "lat= 36.7201600\n",
    "&\n",
    "lng =-4.4203400\n",
    "&\n",
    "date=2021-07-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos los parametros de nuestra query\n",
    "latitud = -34.6\n",
    "longitud = -58.4\n",
    "fecha = '1816-07-09'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el pedido y guardamos la respuesta en una nueva variable\n",
    "import requests\n",
    "respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'sunrise': '10:58:20 AM',\n",
       "  'sunset': '8:58:27 PM',\n",
       "  'solar_noon': '3:58:24 PM',\n",
       "  'day_length': '10:00:07',\n",
       "  'civil_twilight_begin': '10:32:04 AM',\n",
       "  'civil_twilight_end': '9:24:44 PM',\n",
       "  'nautical_twilight_begin': '10:00:49 AM',\n",
       "  'nautical_twilight_end': '9:55:58 PM',\n",
       "  'astronomical_twilight_begin': '9:30:19 AM',\n",
       "  'astronomical_twilight_end': '10:26:29 PM'},\n",
       " 'status': 'OK',\n",
       " 'tzid': 'UTC'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para des-serializar el objetivo (que era tipo 'HTTPResponse') y cargarlo como json \n",
    "datos_sunset =respuesta_sunset.json()\n",
    "datos_sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['results', 'status', 'tzid'])\n",
      "Status: OK\n"
     ]
    }
   ],
   "source": [
    "type(datos_sunset)\n",
    "print(datos_sunset.keys())\n",
    "#Evaluamos el status del pedido\n",
    "sunset_status = datos_sunset['status']\n",
    "print(f'Status: {sunset_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 1816-07-09 el sol se oculto a las 8:58:27 PM (UTC)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver su contenido ya que es son diccionarios anidados \n",
    "sunset= datos_sunset['results']['sunset']\n",
    "print(f'El {fecha} el sol se oculto a las {sunset} (UTC)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion data_sunset['results']:\n",
      "sunrise\n",
      "sunset\n",
      "solar_noon\n",
      "day_length\n",
      "civil_twilight_begin\n",
      "civil_twilight_end\n",
      "nautical_twilight_begin\n",
      "nautical_twilight_end\n",
      "astronomical_twilight_begin\n",
      "astronomical_twilight_end\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteracion data_sunset['results']:\")\n",
    "for elemento in datos_sunset['results']:\n",
    "    print(elemento)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de API por medio de una libreria : Wikipedia\n",
    "Wikipedia-Api es wrapper de Python facil de usar para la API de Wikipedia. Admite la extraccion de textos, secciones, enlaces, categorias, traducciones, etc\n",
    "Repositorio: https://github.com/martin-majilis/Wikipedia-API\n",
    "Documentacion: https://wikipedia-api.readthedocs.io/en/latest/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Programación\n",
      "Resumen: La programación es el proceso de crear un conjunto de instru...\n",
      "Programación\n",
      " \n",
      "La programación es el proceso de crear un conjunto de instrucciones que le dicen a una computadora como realizar algún tipo de tarea. Pero no solo la acción de escribir un código para que la computadora o el software lo ejecute. Incluye, además, todas las tareas necesarias para que el código funcione correctamente y cumpla el objetivo para el cual se escribió.[1]​\n",
      "En la actualidad, la noción de programación se encuentra muy asociada a la creación de aplicaciones de informática y videojuegos. En este sentido, es el proceso por el cual una persona desarrolla un programa, valiéndose de una herramienta que le permita escribir el código (el cual puede estar en uno o varios lenguajes, como C++, Java y Python, entre muchos otros) y de otra que sea capaz de “traducirlo” a lo que se conoce como lenguaje de máquina, que puede \"comprender\" el microprocesador.[2]​\n",
      " \n",
      "https://es.wikipedia.org/wiki/Programaci%C3%B3n\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "user_agent = 'MiAplicacion/1.0 (fernando2017diazescobar@gmail.com)'\n",
    "# Crear un objeto Wikipedia con el idioma deseado y el User-Agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='es',\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "page = wiki_wiki.page('programación')\n",
    "\n",
    "# Verificar si la página existe\n",
    "if page.exists():\n",
    "    # Imprimir el título y el resumen del artículo\n",
    "    print(f\"Título: {page.title}\")\n",
    "    print(f\"Resumen: {page.summary[:60]}...\")  # Mostrar los primeros 60 caracteres del resumen\n",
    "else:\n",
    "    print(\"La página no existe.\")\n",
    "\n",
    "print(page.title)\n",
    "print(' ')\n",
    "print(page.summary)\n",
    "print(' ')\n",
    "print(page.fullurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de BeatifulSoup: 4.12.3\n",
      "Version de BeatifulSoup: 2.32.3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "print(\"Version de BeatifulSoup:\",bs4.__version__)\n",
    "print(\"Version de BeatifulSoup:\",requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en el caso que no pueda se  posible utilizar el modulo \n",
    "### descargar las versiones anteriores de este \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de no tener la version que se usa en este curso\n",
    "#! pip3 install beautifulsoup==4.11.2\n",
    "#! pip3 install requests ==2.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empezamos el scraping\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE ='https://scrapepark.org/courses/spanish'\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "html_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2. \"Parsear\" ese HTML\n",
    "soup = BeautifulSoup(html_obtenido,\"html.parser\")\n",
    "type(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El metodo find()\n",
    "Nos permite quedarnos con la informacion asociada a una etiqueta de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "¿Por qué comprar con nosotros?\n",
      "¿Por qué comprar con nosotros?\n"
     ]
    }
   ],
   "source": [
    "primer_h2 = soup.find('h2')\n",
    "print(primer_h2)\n",
    "#Solo el texto\n",
    "print(primer_h2.text)\n",
    "\n",
    "#equivalente a:\n",
    "print(soup.h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El metodo find_all()\n",
    "Busca TODOS los elementos de la pagina con esa etiqueta y devuelve una \"lista\" que los contiene (en realidad devuelve un objetivo de la clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>, <h2>\n",
      "                  #Novedades\n",
      "                </h2>, <h2>\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>, <h2>\n",
      "            Testimonios de clientes\n",
      "          </h2>, <h2 class=\"heading-container\">\n",
      "          Tabla de precios\n",
      "        </h2>]\n",
      "[<h2>¿Por qué comprar con nosotros?</h2>]\n",
      "¿Por qué comprar con nosotros?\n",
      "\n",
      "                  #Novedades\n",
      "                \n",
      "\n",
      "            Nuestros productos\n",
      "\n",
      "\n",
      "            Testimonios de clientes\n",
      "          \n",
      "\n",
      "          Tabla de precios\n",
      "        \n",
      "\n",
      "\n",
      "¿Por qué comprar con nosotros?\n",
      "#Novedades\n",
      "Nuestrosproductos\n",
      "Testimonios de clientes\n",
      "Tabla de precios\n"
     ]
    }
   ],
   "source": [
    "h2_todos =soup.find_all('h2')\n",
    "print(h2_todos)\n",
    "\n",
    "# si queremos limitar la cantidad de elementos a buscar el argumento limit debe ser ajustado  en un valor a buscar\n",
    "h2_uno_solo = soup.find_all('h2',limit =1)\n",
    "print(h2_uno_solo)\n",
    "\n",
    "# para imprimir solo los textos de los elementos \n",
    "# se puede utilizar un ciclo for \n",
    "for seccion in h2_todos:\n",
    "  print(seccion.text)\n",
    "print('\\n')    \n",
    "  # existe un metodo que permite realizar operaciones mas avanzadas con el texto extraido\n",
    "  # el argumento strip permite limpiar los espacios en blanco del elemento \n",
    "for seccion in h2_todos:\n",
    "    print(seccion.get_text(strip=True))\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando atributos de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"heading-container heading-center\" id=\"acerca\">\n",
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\" id=\"productos\">\n",
      "<h2>\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h3>Suscríbete para obtener descuentos y ofertas</h3>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h2>\n",
      "            Testimonios de clientes\n",
      "          </h2>\n",
      "</div>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "divs =soup.find_all ('div', class_=\"heading-container heading-center\")\n",
    "\n",
    "for div in divs:\n",
    "  print(div)\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"Logo de ScrapePark.org\" src=\"images/logo.svg\" width=\"250\"/>, <img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>, <img alt=\"Variedad de patinetas en la tienda\" src=\"images/arrival-bg-store.png\"/>, <img alt=\"Patineta 1\" src=\"images/p1.png\"/>, <img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>, <img alt=\"Patineta 3\" src=\"images/p3.png\"/>, <img alt=\"Patineta 4\" src=\"images/p4.png\"/>, <img alt=\"Patineta 5\" src=\"images/p5.png\"/>, <img alt=\"Patineta 6\" src=\"images/p6.png\"/>, <img alt=\"Patineta 7\" src=\"images/p7.png\"/>, <img alt=\"Patineta 8\" src=\"images/p8.png\"/>, <img alt=\"Patineta 9\" src=\"images/p9.png\"/>, <img alt=\"Patineta 10\" src=\"images/p10.png\"/>, <img alt=\"Patineta 11\" src=\"images/p11.png\"/>, <img alt=\"Patineta 12\" src=\"images/p12.png\"/>, <img alt=\"Cliente 1\" src=\"images/client-one.png\"/>, <img alt=\"Cliente 2\" src=\"images/client-two.png\"/>, <img alt=\"Cliente 3\" src=\"images/client-three.png\"/>, <iframe src=\"table.html\" title=\"table_iframe\"></iframe>, <img alt=\"#\" src=\"images/logo.svg\" width=\"210\"/>, <img alt=\"Logo de freeCodeCamp\" class=\"freecodecamp-logo\" src=\"./images/freecodecamp-logo.png\"/>, <script src=\"js/jquery-3.4.1.min.js\"></script>, <script src=\"js/popper.min.js\"></script>, <script src=\"js/bootstrap.js\"></script>]\n",
      "<img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>\n",
      "<img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>\n"
     ]
    }
   ],
   "source": [
    "# buscamos todos los elemento con la etiqueta src con el metodo find_all\n",
    "src_todos = soup.find_all(src=True)\n",
    "# este metodo devuelve un lista con el nombre de cada elemento\n",
    "print(src_todos)\n",
    "# con un ciclo for recorremos todos los elementos de la lista\n",
    "for elemento in src_todos:\n",
    "    # si el elemento  termina con \".jpg\" imprimimos el elemento\n",
    "    if elemento['src'].endswith(\".jpg\"):\n",
    "      print(elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/arrival-bg-store.png\n",
      "images/p1.png\n",
      "images/p3.png\n",
      "images/p4.png\n",
      "images/p5.png\n",
      "images/p6.png\n",
      "images/p7.png\n",
      "images/p8.png\n",
      "images/p9.png\n",
      "images/p10.png\n",
      "images/p11.png\n",
      "images/p12.png\n",
      "images/client-one.png\n",
      "images/client-two.png\n",
      "images/client-three.png\n",
      "./images/freecodecamp-logo.png\n"
     ]
    }
   ],
   "source": [
    "#generamos una lista vacia  para almacenar las imagenes \n",
    "url_imagenes=[]\n",
    "# definimos un ciclo for para recorrer los elementos de la lista src_todos\n",
    "for i, imagen in enumerate(src_todos):\n",
    "    # si la imagen termina  con png\n",
    "    if imagen['src'].endswith('png'):\n",
    "        # imprimir la imagen \n",
    "        print(imagen['src'])\n",
    "        #requerir los paquetes de datos de las imagenes\n",
    "        r=requests.get(f\"https://scrapepark.org/spanish/{imagen['src']}\")\n",
    "        # con la funcion open creamos un nuevo archivos con el nombre imagen+ # iteracion\n",
    "        with open(f'imagen_{i}.png','wb') as f:\n",
    "          # almacenamos los paquetes de datos de la imagen con el metodo write \n",
    "          f.write(r.content)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Longboard', '$80', '$85', '$90', '$62', '$150']\n"
     ]
    }
   ],
   "source": [
    "#Informacion de tablas \n",
    "# string que almacena el nombre de  la pagina web\n",
    "URL_BASE = 'https://scrapepark.org/spanish'\n",
    "# lista que almacena los valores de las tablas \n",
    "URL_TABLA = soup.find_all('iframe')[0]['src']\n",
    "# tomar los meta datos de la tablas de la pagina web\n",
    "requests_tabla =requests.get(f'{URL_BASE}/{URL_TABLA}')\n",
    "# definimos una lista con el texto de las tablas \n",
    "html_tabla=requests_tabla.text\n",
    "# alamcena el codigo html de la pagina web\n",
    "soup_tabla =BeautifulSoup(html_tabla,\"html.parser\")\n",
    "# buscar y almacena los elementos con las etiquetas table (las tablas)\n",
    "soup_tabla.find('table')\n",
    "# busca los elementos con las etiquetas th y td y atributos {'style':'color: red;'} dentro de los datos de las tablas \n",
    "productos_faltantes = soup_tabla.find_all(['th','td'],attrs={'style':'color: red;'})\n",
    "# almacena una lista con los textos de los elementos de la tabla \n",
    "productos_faltantes= [talle.text for talle in productos_faltantes]\n",
    "# imprime los elementos extraidos de la tabla \n",
    "print(productos_faltantes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
