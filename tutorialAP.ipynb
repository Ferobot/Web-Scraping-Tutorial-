{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de API de manera directa\n",
    "#### Sunset and sunrise times API\n",
    "Sirve para obtener la hora del amanecer y el ocaso de un determinado dia\n",
    "Parametro:\n",
    "* lat (float):Latitud en grados decimales (Obligatorio)\n",
    "* lng (float):Longitud en grados decimales (Obligatorio)\n",
    "* date(string): Fecha en formato AAAA-MM-DD (opcional, por defecto usa actual)\n",
    "\n",
    "Estructura de la query:\n",
    "\n",
    "https://api.sunrise-sunset.org/json\n",
    "\n",
    "lat= 36.7201600\n",
    "&\n",
    "lng =-4.4203400\n",
    "&\n",
    "date=2021-07-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos los parametros de nuestra query\n",
    "latitud = -34.6\n",
    "longitud = -58.4\n",
    "fecha = '1816-07-09'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el pedido y guardamos la respuesta en una nueva variable\n",
    "import requests\n",
    "respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'sunrise': '10:58:20 AM',\n",
       "  'sunset': '8:58:27 PM',\n",
       "  'solar_noon': '3:58:24 PM',\n",
       "  'day_length': '10:00:07',\n",
       "  'civil_twilight_begin': '10:32:04 AM',\n",
       "  'civil_twilight_end': '9:24:44 PM',\n",
       "  'nautical_twilight_begin': '10:00:49 AM',\n",
       "  'nautical_twilight_end': '9:55:58 PM',\n",
       "  'astronomical_twilight_begin': '9:30:19 AM',\n",
       "  'astronomical_twilight_end': '10:26:29 PM'},\n",
       " 'status': 'OK',\n",
       " 'tzid': 'UTC'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para des-serializar el objetivo (que era tipo 'HTTPResponse') y cargarlo como json \n",
    "datos_sunset =respuesta_sunset.json()\n",
    "datos_sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['results', 'status', 'tzid'])\n",
      "Status: OK\n"
     ]
    }
   ],
   "source": [
    "type(datos_sunset)\n",
    "print(datos_sunset.keys())\n",
    "#Evaluamos el status del pedido\n",
    "sunset_status = datos_sunset['status']\n",
    "print(f'Status: {sunset_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 1816-07-09 el sol se oculto a las 8:58:27 PM (UTC)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver su contenido ya que es son diccionarios anidados \n",
    "sunset= datos_sunset['results']['sunset']\n",
    "print(f'El {fecha} el sol se oculto a las {sunset} (UTC)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion data_sunset['results']:\n",
      "sunrise\n",
      "sunset\n",
      "solar_noon\n",
      "day_length\n",
      "civil_twilight_begin\n",
      "civil_twilight_end\n",
      "nautical_twilight_begin\n",
      "nautical_twilight_end\n",
      "astronomical_twilight_begin\n",
      "astronomical_twilight_end\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteracion data_sunset['results']:\")\n",
    "for elemento in datos_sunset['results']:\n",
    "    print(elemento)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de API por medio de una libreria : Wikipedia\n",
    "Wikipedia-Api es wrapper de Python facil de usar para la API de Wikipedia. Admite la extraccion de textos, secciones, enlaces, categorias, traducciones, etc\n",
    "Repositorio: https://github.com/martin-majilis/Wikipedia-API\n",
    "Documentacion: https://wikipedia-api.readthedocs.io/en/latest/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Programación\n",
      "Resumen: La programación es el proceso de crear un conjunto de instru...\n",
      "Programación\n",
      " \n",
      "La programación es el proceso de crear un conjunto de instrucciones que le dicen a una computadora como realizar algún tipo de tarea. Pero no solo la acción de escribir un código para que la computadora o el software lo ejecute. Incluye, además, todas las tareas necesarias para que el código funcione correctamente y cumpla el objetivo para el cual se escribió.[1]​\n",
      "En la actualidad, la noción de programación se encuentra muy asociada a la creación de aplicaciones de informática y videojuegos. En este sentido, es el proceso por el cual una persona desarrolla un programa, valiéndose de una herramienta que le permita escribir el código (el cual puede estar en uno o varios lenguajes, como C++, Java y Python, entre muchos otros) y de otra que sea capaz de “traducirlo” a lo que se conoce como lenguaje de máquina, que puede \"comprender\" el microprocesador.[2]​\n",
      " \n",
      "https://es.wikipedia.org/wiki/Programaci%C3%B3n\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "user_agent = 'MiAplicacion/1.0 (fernando2017diazescobar@gmail.com)'\n",
    "# Crear un objeto Wikipedia con el idioma deseado y el User-Agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='es',\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "page = wiki_wiki.page('programación')\n",
    "\n",
    "# Verificar si la página existe\n",
    "if page.exists():\n",
    "    # Imprimir el título y el resumen del artículo\n",
    "    print(f\"Título: {page.title}\")\n",
    "    print(f\"Resumen: {page.summary[:60]}...\")  # Mostrar los primeros 60 caracteres del resumen\n",
    "else:\n",
    "    print(\"La página no existe.\")\n",
    "\n",
    "print(page.title)\n",
    "print(' ')\n",
    "print(page.summary)\n",
    "print(' ')\n",
    "print(page.fullurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de BeatifulSoup: 4.12.3\n",
      "Version de BeatifulSoup: 2.32.3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "print(\"Version de BeatifulSoup:\",bs4.__version__)\n",
    "print(\"Version de BeatifulSoup:\",requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en el caso que no pueda se  posible utilizar el modulo \n",
    "### descargar las versiones anteriores de este \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de no tener la version que se usa en este curso\n",
    "#! pip3 install beautifulsoup==4.11.2\n",
    "#! pip3 install requests ==2.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empezamos el scraping\n",
    "\n",
    "# 1. Obtener el HTML\n",
    "URL_BASE ='https://scrapepark.org/courses/spanish'\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "html_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2. \"Parsear\" ese HTML\n",
    "soup = BeautifulSoup(html_obtenido,\"html.parser\")\n",
    "type(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### El metodo find()\n",
    "Nos permite quedarnos con la informacion asociada a una etiqueta de HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "¿Por qué comprar con nosotros?\n",
      "¿Por qué comprar con nosotros?\n"
     ]
    }
   ],
   "source": [
    "primer_h2 = soup.find('h2')\n",
    "print(primer_h2)\n",
    "#Solo el texto\n",
    "print(primer_h2.text)\n",
    "\n",
    "#equivalente a:\n",
    "print(soup.h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El metodo find_all()\n",
    "Busca TODOS los elementos de la pagina con esa etiqueta y devuelve una \"lista\" que los contiene (en realidad devuelve un objetivo de la clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>¿Por qué comprar con nosotros?</h2>, <h2>\n",
      "                  #Novedades\n",
      "                </h2>, <h2>\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>, <h2>\n",
      "            Testimonios de clientes\n",
      "          </h2>, <h2 class=\"heading-container\">\n",
      "          Tabla de precios\n",
      "        </h2>]\n",
      "[<h2>¿Por qué comprar con nosotros?</h2>]\n",
      "¿Por qué comprar con nosotros?\n",
      "\n",
      "                  #Novedades\n",
      "                \n",
      "\n",
      "            Nuestros productos\n",
      "\n",
      "\n",
      "            Testimonios de clientes\n",
      "          \n",
      "\n",
      "          Tabla de precios\n",
      "        \n",
      "\n",
      "\n",
      "¿Por qué comprar con nosotros?\n",
      "#Novedades\n",
      "Nuestrosproductos\n",
      "Testimonios de clientes\n",
      "Tabla de precios\n"
     ]
    }
   ],
   "source": [
    "h2_todos =soup.find_all('h2')\n",
    "print(h2_todos)\n",
    "\n",
    "# si queremos limitar la cantidad de elementos a buscar el argumento limit debe ser ajustado  en un valor a buscar\n",
    "h2_uno_solo = soup.find_all('h2',limit =1)\n",
    "print(h2_uno_solo)\n",
    "\n",
    "# para imprimir solo los textos de los elementos \n",
    "# se puede utilizar un ciclo for \n",
    "for seccion in h2_todos:\n",
    "  print(seccion.text)\n",
    "print('\\n')    \n",
    "  # existe un metodo que permite realizar operaciones mas avanzadas con el texto extraido\n",
    "  # el argumento strip permite limpiar los espacios en blanco del elemento \n",
    "for seccion in h2_todos:\n",
    "    print(seccion.get_text(strip=True))\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando atributos de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"heading-container heading-center\" id=\"acerca\">\n",
      "<h2>¿Por qué comprar con nosotros?</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\" id=\"productos\">\n",
      "<h2>\n",
      "            Nuestros <span>productos</span>\n",
      "</h2>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h3>Suscríbete para obtener descuentos y ofertas</h3>\n",
      "</div>\n",
      " \n",
      "<div class=\"heading-container heading-center\">\n",
      "<h2>\n",
      "            Testimonios de clientes\n",
      "          </h2>\n",
      "</div>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "divs =soup.find_all ('div', class_=\"heading-container heading-center\")\n",
    "\n",
    "for div in divs:\n",
    "  print(div)\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<img alt=\"Logo de ScrapePark.org\" src=\"images/logo.svg\" width=\"250\"/>, <img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>, <img alt=\"Variedad de patinetas en la tienda\" src=\"images/arrival-bg-store.png\"/>, <img alt=\"Patineta 1\" src=\"images/p1.png\"/>, <img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>, <img alt=\"Patineta 3\" src=\"images/p3.png\"/>, <img alt=\"Patineta 4\" src=\"images/p4.png\"/>, <img alt=\"Patineta 5\" src=\"images/p5.png\"/>, <img alt=\"Patineta 6\" src=\"images/p6.png\"/>, <img alt=\"Patineta 7\" src=\"images/p7.png\"/>, <img alt=\"Patineta 8\" src=\"images/p8.png\"/>, <img alt=\"Patineta 9\" src=\"images/p9.png\"/>, <img alt=\"Patineta 10\" src=\"images/p10.png\"/>, <img alt=\"Patineta 11\" src=\"images/p11.png\"/>, <img alt=\"Patineta 12\" src=\"images/p12.png\"/>, <img alt=\"Cliente 1\" src=\"images/client-one.png\"/>, <img alt=\"Cliente 2\" src=\"images/client-two.png\"/>, <img alt=\"Cliente 3\" src=\"images/client-three.png\"/>, <iframe src=\"table.html\" title=\"table_iframe\"></iframe>, <img alt=\"#\" src=\"images/logo.svg\" width=\"210\"/>, <img alt=\"Logo de freeCodeCamp\" class=\"freecodecamp-logo\" src=\"./images/freecodecamp-logo.png\"/>, <script src=\"js/jquery-3.4.1.min.js\"></script>, <script src=\"js/popper.min.js\"></script>, <script src=\"js/bootstrap.js\"></script>]\n",
      "<img alt=\"Parque de patinaje\" src=\"images/slider-bg.jpg\"/>\n",
      "<img alt=\"Patineta 2\" src=\"images/p2.jpg\"/>\n"
     ]
    }
   ],
   "source": [
    "# buscamos todos los elemento con la etiqueta src con el metodo find_all\n",
    "src_todos = soup.find_all(src=True)\n",
    "# este metodo devuelve un lista con el nombre de cada elemento\n",
    "print(src_todos)\n",
    "# con un ciclo for recorremos todos los elementos de la lista\n",
    "for elemento in src_todos:\n",
    "    # si el elemento  termina con \".jpg\" imprimimos el elemento\n",
    "    if elemento['src'].endswith(\".jpg\"):\n",
    "      print(elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/arrival-bg-store.png\n",
      "images/p1.png\n",
      "images/p3.png\n",
      "images/p4.png\n",
      "images/p5.png\n",
      "images/p6.png\n",
      "images/p7.png\n",
      "images/p8.png\n",
      "images/p9.png\n",
      "images/p10.png\n",
      "images/p11.png\n",
      "images/p12.png\n",
      "images/client-one.png\n",
      "images/client-two.png\n",
      "images/client-three.png\n",
      "./images/freecodecamp-logo.png\n"
     ]
    }
   ],
   "source": [
    "#generamos una lista vacia  para almacenar las imagenes \n",
    "url_imagenes=[]\n",
    "# definimos un ciclo for para recorrer los elementos de la lista src_todos\n",
    "for i, imagen in enumerate(src_todos):\n",
    "    # si la imagen termina  con png\n",
    "    if imagen['src'].endswith('png'):\n",
    "        # imprimir la imagen \n",
    "        print(imagen['src'])\n",
    "        #requerir los paquetes de datos de las imagenes\n",
    "        r=requests.get(f\"https://scrapepark.org/spanish/{imagen['src']}\")\n",
    "        # con la funcion open creamos un nuevo archivos con el nombre imagen+ # iteracion\n",
    "        with open(f'imagen_{i}.png','wb') as f:\n",
    "          # almacenamos los paquetes de datos de la imagen con el metodo write \n",
    "          f.write(r.content)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Longboard', '$80', '$85', '$90', '$62', '$150']\n"
     ]
    }
   ],
   "source": [
    "#Informacion de tablas \n",
    "# string que almacena el nombre de  la pagina web\n",
    "URL_BASE = 'https://scrapepark.org/spanish'\n",
    "# lista que almacena los valores de las tablas \n",
    "URL_TABLA = soup.find_all('iframe')[0]['src']\n",
    "# tomar los meta datos de la tablas de la pagina web\n",
    "requests_tabla =requests.get(f'{URL_BASE}/{URL_TABLA}')\n",
    "# definimos una lista con el texto de las tablas \n",
    "html_tabla=requests_tabla.text\n",
    "# alamcena el codigo html de la pagina web\n",
    "soup_tabla =BeautifulSoup(html_tabla,\"html.parser\")\n",
    "# buscar y almacena los elementos con las etiquetas table (las tablas)\n",
    "soup_tabla.find('table')\n",
    "# busca los elementos con las etiquetas th y td y atributos {'style':'color: red;'} dentro de los datos de las tablas \n",
    "productos_faltantes = soup_tabla.find_all(['th','td'],attrs={'style':'color: red;'})\n",
    "# almacena una lista con los textos de los elementos de la tabla \n",
    "productos_faltantes= [talle.text for talle in productos_faltantes]\n",
    "# imprime los elementos extraidos de la tabla \n",
    "print(productos_faltantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producto: Patineta Nueva 1 | precio: 75\n",
      "producto: Patineta Usada 2 | precio: 80\n",
      "producto: Patineta Nueva 3 | precio: 68\n",
      "producto: Patineta Usada 4 | precio: 70\n",
      "producto: Patineta Nueva 5 | precio: 75\n",
      "producto: Patineta Nueva 6 | precio: 58\n",
      "producto: Patineta Nueva 7 | precio: 80\n",
      "producto: Patineta Nueva 8 | precio: 35\n",
      "producto: Patineta Nueva 9 | precio: 165\n",
      "producto: Patineta Usada 10 | precio: 54\n",
      "producto: Patineta Usada 11 | precio: 99\n",
      "producto: Patineta Nueva 12 | precio: 110\n"
     ]
    }
   ],
   "source": [
    "# buscar todos los elementos de tipo divs y clase detail-box\n",
    "divs =soup.find_all('div',class_ ='detail-box') \n",
    "# lista vacia para almacenar los elementos de tipo producto\n",
    "productos = [] \n",
    "# lista vacia para almacenar los elementos de tipo precios\n",
    "precios =[]\n",
    "#generamos un ciclo for para recorres los elementos de la lista divs\n",
    "for div in divs:\n",
    "    # si el valor de h6 del div no esta vacio y el texto patineta esta en div.h5.text   \n",
    "    if (div.h6 is not None) and ('Patineta' in div.h5.text):\n",
    "        # almacenar el nombre del producto en la variable producto\n",
    "        producto = div.h5.get_text(strip =True)\n",
    "        # almacenar el valor del producto en la variable precio y sustituir el signo precio por un espacio en blanco\n",
    "        precio = div.h6.get_text(strip =True).replace('$','')\n",
    "        # Se puede agregar filtros\n",
    "        # imprimir el valor del producto si este tiene un valor menor a 16  y el precio del producto\n",
    "        print(f'producto: {producto:<16} | precio: {precio}')\n",
    "        # agregar el producto a la lista de productos\n",
    "        productos.append(producto)\n",
    "        # agregar el precio a la lista de precios \n",
    "        precios.append(precio)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cambios que dependen de la URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scrapepark.org/spanish/contact1\n",
      "Texto que cambia entre páginas en contacto 1 :)\n",
      "https://scrapepark.org/spanish/contact2\n",
      "Texto que cambia entre páginas en contacto 2 :)\n"
     ]
    }
   ],
   "source": [
    "# definimos un string base para iterrar entre las url\n",
    "URL_BASE = \"https://scrapepark.org/spanish/contact\"\n",
    "# definimos un ciclo for para recorrer las url \n",
    "for i in range(1,3):\n",
    "    # la url de la iteracion \n",
    "    URL_FINAL = f\"{URL_BASE}{i}\"\n",
    "    # imprimer el nombre de la url de la iteracion\n",
    "    print(URL_FINAL)\n",
    "    # obtener los paquetes de datos de la url\n",
    "    r =requests.get(URL_FINAL)\n",
    "    # generar una instancia con los paquetes de datos de la url\n",
    "    soup =BeautifulSoup(r.text,\"html.parser\")\n",
    "    #imprimir el texto del elementos h5\n",
    "    print(soup.h5.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[': 4-444-4444']\n"
     ]
    }
   ],
   "source": [
    "#Expresiones regulares \n",
    "# importamos el modulo de expresiones regulares\n",
    "import re\n",
    "\n",
    "#1. Obtener el html\n",
    "#definimos un string que almacena la direccion web\n",
    "URL_BASE = 'https://scrapepark.org/spanish'\n",
    "#tomamos los paquetes de datos de la pagina web\n",
    "pedido_obtenido = requests.get(URL_BASE)\n",
    "#definimos una variable para almacenar el texto de la pagina web\n",
    "html_obtenido = pedido_obtenido.text\n",
    "\n",
    "# 2.Parsear ese HTML\n",
    "soup =BeautifulSoup(html_obtenido, \"html.parser\")\n",
    "\n",
    "# utilizando el metodo find_all y el metodo compile buscamos la palabra\n",
    "# el metodo compile permite identificar el tipo de elemento que es el tipo de texto que buscamos\n",
    "# luego find all nos devuelve un iterrable con todos los elementos queen su text tenga la forma buscada  \n",
    "telefonos =soup.find_all(string=re.compile(\"\\d+-\\d+-\\d+\"))\n",
    "print(telefonos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['© 2022 ']\n"
     ]
    }
   ],
   "source": [
    "copyrights=soup.find_all(string=re.compile(\"©\"))\n",
    "print(copyrights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>© 2022 <span>Todos los derechos reservados</span>.\n",
      "        <a href=\"https://html.design/\" rel=\"noopener noreferrer\" target=\"_blank\">Creado con Free Html Templates</a>.\n",
      "      </p>\n"
     ]
    }
   ],
   "source": [
    "# para poder aceder a todo los datos de la etiqueta padre \n",
    "primer_copyright = copyrights[0]\n",
    "print (primer_copyright.parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul>\n",
       " <li><a href=\"#\">Inicio</a></li>\n",
       " <li><a href=\"#\">Acerca</a></li>\n",
       " <li><a href=\"#\">Servicios</a></li>\n",
       " <li><a href=\"#\">Testimonios</a></li>\n",
       " <li><a href=\"#\">Contacto</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menu = soup.find(string=re.compile(\"MENÚ\"))\n",
    "#menu.parent\n",
    "# perimite identificar todos los elementos que estan al mismo nivel \n",
    "menu.parent.find_next_siblings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comentario sobre excepciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENÚ\n",
      "© 2022 \n",
      "El string 'carpincho' no fue encontrado\n",
      "Patineta nueva\n"
     ]
    }
   ],
   "source": [
    "strings_a_buscar =[\"MENÚ\",\"©\",\"carpincho\",\"Patineta\"]\n",
    "\n",
    "for string in strings_a_buscar :\n",
    "    try: \n",
    "        resultado = soup.find(string=re.compile(string))\n",
    "        print(resultado.text)\n",
    "    except AttributeError:\n",
    "        print(f\"El string '{string}' no fue encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenamiento de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# este modulo permite trabajar con archivos csv\n",
    "import csv\n",
    "# insertamos la palabra \"producto como primer elemento de la lista Productos\"\n",
    "productos.insert(0,\"productos\")\n",
    "# insertamos la palabra \"precio como primer elemento de la lista Productos\"\n",
    "precios.insert(0,\"precios\")\n",
    "# con la funcion dict genereramos un diccionario  con  la tubla que devuelve la fincion zip, la funcion zip utiliza los elementos\n",
    "# de las listas que tiene como argumento \n",
    "datos = dict(zip(productos,precios))\n",
    "# con la funcion open creamos o modificamos un archivo csv, y le asignamos el objeto f\n",
    "with open('datos.csv','w') as f:\n",
    "    #con el metodo writer abrimos el archivo \n",
    "    W=csv.writer(f)\n",
    "    # almacenamos los datoos con la funcion writerows\n",
    "    W.writerows(datos.items())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
